---
title: "ECON 773: Assignment 2"
author: "The BLUE Team, Jeneta Ljutic (400138620), Tadhg Taylor-McGreal (400330297), Stella Till (400364649)"
date: "January 19, 2026"
format:
  typst: 
    toc: TRUE
---

{{< pagebreak >}} 

# Preface

## Goal

The goals of this assignment are to: 

- explore the properties of the OLS estimator using Monte Carlo experiments
- analyze data from an RCT using `lm_robust` and `lm_lin`

## Instructions

See assignment 1.

# Monte Carlo experiments

## Monte Carlo 1: coin flips

This question will be demonstrated in class, as an introduction to Monte Carlo experiments.

Flip a coin once (`1` for heads, `0` for tails), and store in `X_1`:

```{r}
(X_1 <- sample(0:1, size = 1, replace = TRUE))
```

The variable `n` will refer to sample size.
Roll a dice `n` times:

```{r}
n <- 10
(X_n <- sample(0:1, size = n, replace = TRUE))
```

We will use a large set of packages for this assignment, loaded in the next chunk. Do you recognize them?

```{r}
#| eval: false
install.packages("gtsummary")
install.packages("gt")
install.packages("gtsummary")
install.packages("estimatr")
install.packages("AER")
```

```{r}
library(tidyverse)
library(gt)
library(gtsummary)
library(estimatr)
library(broom)
```

To compute the mean up to and including the $i$th throw, we can use the `cummean` function from `dplyr`:

```{r}
Xbar_in <- cummean(X_n)
cbind(X_n, Xbar_in)
```

If you only want the mean across all `n` flips, can do:

```{r}
(Xbar_n <- mean(sample(0:1, size = n, replace = TRUE)))
```

We can use the following function to generate a tibble that keeps track across flips:

```{r}
gen_coin_cm <- function(n, experiment) {
    X_n <- sample(0:1, size = n, replace = TRUE)
    Xbar_in <- cummean(X_n)
    return(tibble(i = 1:n, X_n = X_n, Xbar_in = Xbar_in, experiment = experiment))
}
```

Once you define a function, you can use it as follows:

```{r}
gen_coin_cm(20, 3)
```

First, generate a tibble `coin_df` with `n = 10` and `experiment = 1`.
Second, make a plot of `coin_df` with `i` on the horizontal and `Xbar_in` on the vertical.
Third, set `n = 1000`, generate `coin1_df` as in part 1 of this question, and redo the plot in part 2.
Fourth, generate additional data sets `coin2_df` and `coin3_df` as in part 3 of this question. 
Join them all together using `bind_rows`.
Then, make a line plot with a different colour for each experiment, using `colour = factor(experiment)` in `geom_line`.
Fifth, make one more plot with one experiment and `n = 100000`.

Interpret your results. Is the phenomenon you observe related to the LLN or the CLT?

### Answer

We are going to use the 'gen_coin_cm' function that is given in the question, to simulate 10 coin flips from experiment 1.
...

```{r}
coin_df <- gen_coin_cm(10,1)
```
In order to see whether this worked, we now make the plot requested in the second part of this question. 

```{r}
coin_df |> ggplot() + 
    geom_point(aes(x=i, y=Xbar_in)) + 
    geom_line(aes(x=i, y=Xbar_in))
```

To compelte the third part of this question, we are going to copy, paste and modifiy the code from above. 

```{r}
coin1_df <- gen_coin_cm(1000,1)
coin1_df |> ggplot() + 
    geom_line(aes(x=i, y=Xbar_in))
```

Next, we repeat this experiment twice more. 

```{r}
coin2_df <- gen_coin_cm(1000,2)
coin3_df <- gen_coin_cm(1000,3)
```

Put all the experimental data on a big pile. 

```{r}
all_experiments <- bind_rows(
    coin1_df, coin2_df, coin3_df
)
```

Now we are ready to make the final plot of this question. 

```{r}
all_experiments |> ggplot() + 
    geom_line(aes(x=i, y=Xbar_in, colour=factor(experiment)))
```

```{r}
big_coin_df <- gen_coin_cm
(10000, 1)
big_coin_df |> ggplot() + 
    geom_line(aes(x=i, y=Xbar_in))
```

## Monte Carlo 2: dice throws

In the previous exercise, we fixed `S` and let `n` grow large.
Let us now slowly grow `n` and see what happens, letting `S` be very large.
We start with `n = 1`, corresponding to one roll of the die.
(For this second exercise, we will use a die instead of a coin.)

One roll:

```{r}
S <- 100
x <- sample(1:6, S, replace = TRUE)
die_df <- tibble(s = 1:S, Xbar = x)
die_df |> ggplot(aes(x = Xbar)) +
    geom_histogram(binwidth = 1)
```

Let us redo this with larger `S`.

```{r}
S <- 100000
x <- sample(1:6, S, replace = TRUE)
die_df <- tibble(s = 1:S, Xbar = x)
die_df |> ggplot(aes(x = Xbar)) +
    geom_histogram(binwidth = 1)
```

A large `S` seems to be closer to the population probabilities of $P(X=x) = 1/6$.
Because our goal is to explore the theoretical properties of random variables, we will stick with large `S`.

Roll twice and average.

```{r}
n <- 2
x <- 1 / n * (sample(1:6, S, replace = TRUE) + sample(1:6, S, replace = TRUE))
die_df <- tibble(s = 1:S, Xbar = x)
die_df |> ggplot(aes(x = Xbar)) +
    geom_histogram(binwidth = 1 / n)
```

Roll three times and average.

```{r}
n <- 3
x <- 1 / n * sample(1:6, S, replace = TRUE)
for (k in 2:n) {
    x <- x + 1 / n * sample(1:6, S, replace = TRUE)
}
die_df <- tibble(s = 1:S, Xbar = x)
die_df |> ggplot(aes(x = Xbar)) +
    geom_histogram(binwidth = 1 / n)
```

Experiment with a few more values of `n` (let `n = 50` be your largest one.)

First: what happens to the variance of the mean as `n` increases?
Second: what happens to the shape of the distribution as `n` increases? Relate your answer to the Central Limit Theorem (CLT).

### Answer

...


{{< pagebreak >}}

## Monte Carlo 3: Inference in RCTs

This question will be solved in class.

We want to understand the properties of different estimators in an RCT.
We consider a setup where the treatment effect varies with a covariate $X$, and treatment assignment is unbalanced.

-   $X_i \in \{0, 1\}$ with $P(X_i=1) = 0.5$
-   $D_i \in \{0, 1\}$ with $P(D_i=1 \mid X_i) = 0.2$
-   Outcomes are generated as:
    $$Y_i = 0 + 1 \cdot D_i + 5 \cdot X_i + 5 \cdot (D_i \times X_i) + \epsilon_i$$
    where $\epsilon_i \sim N(0, (1 + 2X_i)^2)$.

Calculate the ATE and the CATEs for $X=0$ and $X=1$ based on these parameters.

We want to see if the reported standard errors from simple linear regressions are trustworthy.
We compare two common approaches:

1.  DIM: `lm(Y ~ D)`.
2.  Additive: `lm(Y ~ D + X)`.

We run a simulation to check:

1.  Bias: Is the average estimate close to the true ATE?
2.  Efficiency: What is the standard deviation of the estimates?
3.  Coverage Probability: How often does the 95% Confidence Interval contain the true ATE?

Consider the following function that performs one simulation replication and returns a tibble with the results for both estimators:

```{r}
dgp_het <- function(n) {
    D <- rbinom(n, 1, 0.2) # Unbalanced assignment
    X <- rbinom(n, 1, 0.5)
    e <- rnorm(n, 0, 1 + 2 * X) # Heteroskedastic errors
    Y <- 0 + 1 * D + 5 * X + 5 * D * X + e
    return(tibble(D = D, X = X, Y = Y))
}

sim_one <- function(s, n) {
    df <- dgp_het(n)
    truth <- 3.5

    # 1. DIM
    fit_dim <- lm(Y ~ D, data = df)
    res_dim <- tidy(fit_dim, conf.int = TRUE) |>
        filter(term == "D") |>
        mutate(estimator = "DIM", covered = (conf.low < truth & conf.high > truth))

    # 2. Additive
    fit_add <- lm(Y ~ D + X, data = df)
    res_add <- tidy(fit_add, conf.int = TRUE) |>
        filter(term == "D") |>
        mutate(
            estimator = "Additive",
            covered = (conf.low < truth & conf.high > truth)
        )

    bind_rows(res_dim, res_add) |>
        mutate(s = s)
}

sim_one(1, 200)
```

Your task is to:

1. Run this simulation $S=1000$ times with $n=200$.
2. Summarize the results: calculate the mean estimate, standard deviation of the estimate, and mean coverage probability for each estimator.
3. Interpret the findings related to bias and efficiency of the estimators: which estimator do you prefer?
4. Interpret the findings related to coverage probability: do you trust the confidence intervals from these estimators?

### Answer

```{r}
S <- 1000
test_df <- 1:S |> map_df(sim_one, n=200)

test_df |> group_by(estimator) |> 
        summarize(
            mean_ATE_hat = mean(estimate),
            sd_ATE_hat = sd(estimate),
            coverage = mean(covered)
        )
```

Commentary on the later part of the questions. 
...


{{< pagebreak >}}

## Monte Carlo 4: Lin's estimator

Now it is your turn.

Replicate the simulation from Monte Carlo 3 but add a third estimator: Lin's Estimator.
Use `lm_lin(Y ~ D, covariates = ~ X)`.
You should then be able to extract the estimator information using something like:

```{r}
#| eval: false
est_lin <- lm_lin(Y ~ D, covariates = ~X, data = df)
res_lin <- tidy(est_lin) |>
    filter(term == "D") |>
    mutate(estimator = "Lin", covered = (conf.low < truth & conf.high > truth))
```

Questions:

1.  Write a function `sim_lin_ext(s, n)` that returns a tibble with results for DIM, Additive, and Lin.
2.  Calculate the mean estimate, standard deviation, and coverage probability for all three.
3.  Does Lin's estimator achieve the correct coverage?
4.  Which estimator has the lowest variance?

Use $S=100$ and $n=1000$.

### Answer

1. Start off by writing a function that combines reults from the DIM, Additive and the new Lin model. 
```{r}
sim_lin_ext <- function(s, n) {
  df <- dgp_het(n)
  truth <- 3.5
  
  # 1. DIM
  fit_dim <- lm(Y ~ D, data = df)
  res_dim <- tidy(fit_dim, conf.int = TRUE) |>
    filter(term == "D") |>
    mutate(
      estimator = "DIM",
      covered = (conf.low < truth & conf.high > truth)
    )
  
  # 2. Additive
  fit_add <- lm(Y ~ D + X, data = df)
  res_add <- tidy(fit_add, conf.int = TRUE) |>
    filter(term == "D") |>
    mutate(
      estimator = "Additive",
      covered = (conf.low < truth & conf.high > truth)
    )
  
  # 3. Lin
  fit_lin <- lm_lin(Y ~ D, covariates = ~ X, data = df)
  res_lin <- tidy(fit_lin, conf.int = TRUE) |>
    filter(term == "D") |>
    mutate(
      estimator = "Lin",
      covered = (conf.low < truth & conf.high > truth)
    )
  
  bind_rows(res_dim, res_add, res_lin) |>
    mutate(s = s)
}
```

When called, this function will return a simulation of each model that returns a tibble with results for DIM, Additive, and Lin.

2. We will run the simulation S= 100 times with n = 1000 and calculate the mean estimate, standard deviation, and coverage probability for all three model versions.

```{r}
S <- 100     # number of replications
n <- 1000    # sample size per replication

set.seed(123)

sim_results <- 1:S |> 
  map_df(sim_lin_ext, n = n)

summary_results <- sim_results |>
  group_by(estimator) |>
  summarise(
    mean_ATE_hat = mean(estimate),
    sd_ATE_hat = sd(estimate),
    coverage = mean(covered),
    .groups = "drop"
  )

summary_resultsS <- 100     # number of Monte Carlo replications
n <- 1000    # sample size per replication

set.seed(123)

sim_results <- 1:S |> 
  map_df(sim_lin_ext, n = n)

summary_results <- sim_results |>
  group_by(estimator) |>
  summarise(
    mean_ATE_hat = mean(estimate),
    sd_ATE_hat = sd(estimate),
    coverage = mean(covered),
    .groups = "drop"
  )

summary_results
```

'summary_results' returns a table with the mean estimate, standard deviation, and coverage probability for all three models. 

3. Lin’s coverage is 0.92, which is still slightly below the nominal 0.95, technically it does not achieve correct coverage. This is however likely due to the unbalanced assignment (20% treated) and the heteroskedasticity: the combination of unbalanced groups and high noise means that n=1000 is actually still a "small" sample for these robust calculations. In larger simulations (here we only have S=100) or larger samples, Lin's estimator consistently hits the 95% target.

4. Lin’s estimator has the lowest variance at 0.188. Lin’s estimator is therefore the most efficient, producing the most precise ATE estimates among the three versions. 
...


{{< pagebreak >}}

## Monte Carlo 5, Bonus: Selection Bias

Imagine a scenario where the probability of treatment depends on $X$.

-   $X \in \{0, 1\}$ with $P(X=1) = 0.5$.
-   Stratified Assignment:
    -   If $X=0$, $P(D=1) = 0.2$.
    -   If $X=1$, $P(D=1) = 0.8$.

-   Outcome: $Y = 0 + 1 \cdot D + 5 \cdot X + 2 \cdot D \cdot X + \epsilon$.
    
So $X=1$ makes you more likely to be treated AND increases your outcome.
Treatment effect is also larger for $X=1$.

Questions:

1.  Write a function `dgp_selection(n)` for this design.
2.  Run a simulation ($S=1000, n=200$) comparing:

    -   DIM: `lm(Y ~ D)`
    -   Lin: `lm_lin(Y ~ D, covariates = ~ X)`

3.  Show that DIM is biased. Calculate the bias.
4.  Show that Lin recovers the true ATE.
5.  Why does DIM fail here?

### Answer

...


{{< pagebreak >}}

# Linear regression for experiments

## Job corps, reanalysis

This question will be demonstrated in class.

We will use the same data as in the "Job corps" question in Assignment 1:

```{r}
library(causalweight)
data(JC)
JC <- as_tibble(JC)
```

Here is one way to compute the difference in means of the treatment and control group.
compare it to the result on H, p. 21.

```{r}
JC |> filter(assignment == 1) -> JC_short_TG
JC |> filter(assignment == 0) -> JC_short_CG
treat_mean <- mean(JC_short_TG$earny4)
control_mean <- mean(JC_short_CG$earny4)
(ATE_hat <- treat_mean - control_mean)
```

That is identical to the estimated effect in H3.1.

Questions:

1.  Estimate the ATE using `lm_robust` and present the results in a regression table using `tbl_regression`.
2.  Run a regression that reveals whether the effect is different for men and women. Use `lm_lin` with the `covariates` argument.

Interpret all your results.

### Answer

...


{{< pagebreak >}}

## HIV RCT, reanalysis

We will analyze the effect of the HIV information campaign in Thornton (2008).

After you complete the data analysis below, please also answer the following substantive questions:

1. What exactly is the specific policy intervention studied in this paper?
2. Can you think of alternative policy interventions that might achieve similar goals?
3. Based on the results, is this an effective policy? What would you recommend to the health authority in Malawi?
4. Would you recommend this policy to the Canadian health authorities?
5. What is your main criticism of Thornton's study?

We first need to conduct the empirical analysis. 
Use the same steps as in the previous exercise to analyze the effect of the campaign (`any`). Use `lm_robust` and `tbl_regression`.
Explore whether the treatment effect differs by age (heterogeneity), by creating a binary variable for age (e.g. median split or `age >= 33`) and using `lm_lin`.

Remember to use `drop_na` after loading the data to remove the missing data.

### Answer

...


{{< pagebreak >}} 

## Project STAR

When loading libraries, we sometimes want to suppress messages and warnings to keep our document clean. We can do this by setting `#| message: false` and `#| warning: false` in the chunk options.

Load the `AER` library (install first, if necessary), and the `STAR` data that comes with it:

```{r}
#| message: false
#| warning: false
library(AER)
data("STAR")
STAR <- as_tibble(STAR) |> drop_na()
```

From `?STAR` (which you should look at, to find the variable descriptions):

> Project STAR (Student/Teacher Achievement Ratio) was a four-year longitudinal class-size study funded by the Tennessee General Assembly and conducted in the late 1980s by the State Department of Education. Over 7,000 students in 79 schools were randomly assigned into one of three interventions: small class (13 to 17 students per teacher), regular class (22 to 25 students per teacher), and regular-with-aide class (22 to 25 students with a full-time teacher's aide). Classroom teachers were also randomly assigned to the classes they would teach. The interventions were initiated as the students entered school in kindergarten and continued through third grade.

We will focus on 3rd grade effects. We summarize the treatment variable using `tbl_summary` from the `gtsummary` package for a nicer look:

```{r}
STAR |>
    select(star3) |>
    tbl_summary()
```

We first define the treatment variable as 0 if class is "regular", 1 if small:

```{r}
STAR |>
    filter(star3 %in% c("regular", "small")) |>
    mutate(D3 = ifelse(star3 == "small", 1, 0)) -> STAR_binary
```

First, compute the treatment effect of `D3` on total reading scaled score in 3rd grade. 
Make sure to use heteroskedasticity-robust standard errors throughout. Interpret the results.
Second, compute the effect on total math scaled score in 3rd grade. Interpret the results.

Third, run the regression for math using "multiple treatments", so using `star3` as the treatment variable. 
Make sure to use `STAR`, not `STAR_binary`.
Do you find the same results for the `small` treatment?
Interpret your results for the `regular+aide` treatment.

Bonus points: can you find evidence of heterogeneity of the treatment effect?

### Answer

Checking the variables of the data set

```{r}
?STAR
```

1. Using 'lm_robust' (which builds heteroskedasticity-robust inference in by default), we will compute the treatment effect of 'D3' on the total reading scaled score in the 3rd grade, 'read3'. 

```{r}
library(estimatr)

read3_mod_robust <- lm_robust(
  read3 ~ D3,
  data = STAR_binary,
  se_type = "HC1"   # heteroskedasticity-robust SEs
)

summary(read3_mod_robust)
```

The coefficient on D3 is 5.107 and the p-value is less than 0.05. Therefore, using heteroskedasticity-robust standard errors, assignment to a small class increases 3rd-grade reading scores by about 5 points relative to regular classes, and the effect is statistically significant at the 5% level. 

2. Next, we will compute the effect on total math scale scored in the 3rd grade, 'math3'. This is very similar to the code above.

```{r}
math3_mod <- lm_robust(
  math3 ~ D3,
  data = STAR_binary,
  se_type = "HC1"
)

summary(math3_mod)
```

The coefficient on D3 is 3.075, but the p-value is high at 0.1519. Assignment to a small class therefore increases 3rd-grade math scores by about 3 points, but the effect is not statistically significant, with a 95% confidence interval that includes zero.

3. Lastly, we will run the regression for math using multiple treatments with `star3` as the treatment variable. Each treatment is encoded as a binary indicator (R handles factor variables as binary in regression by default). The intercept corresponds to the omitted (baseline) category ("regular") and each coefficient compares its group to the reference group. 

```{r}
math3_multi <- lm_robust(
  math3 ~ star3,
  data = STAR,
  se_type = "HC1"
)

summary(math3_multi)
```

The effect of being in a small class is positive (3.075 points) and unchanged whether we use the binary or multiple-treatment specification, and the standard error is the same. This is because "regular" remains the omitted reference category in both models. 

The effect of being in a regular class with an aide is small (around 1.3 points) and also not statistically significant, indicating that there is no evidence that adding a full-time aide to a regular-sized class improves 3rd-grade math scores.

Bonus: we can explore whether the effect of small class sizes varies across subgroups (hetergenous treatment effects). For example, we can see whether the effect of small class sizes differs depending on student characteristics like gender, SES or teachers characteristics like experience teaching, etc.

We'll start by observing the effect across gender. 

```{r}
# Heterogeneity by gender - reading scores
read3_gender_mod <- lm_robust(
  read3 ~ D3 * gender,
  data = STAR_binary,
  se_type = "HC1"
)

summary(read3_gender_mod)

# Heterogeneity by gender - math score
math3_gender_mod <- lm_robust(
  math3 ~ D3 * gender,
  data = STAR_binary,
  se_type = "HC1"
)

summary(read3_gender_mod)
summary(math3_gender_mod)

```

There is no evidence of heterogeneity of the small-class effect by gender: small classes increase reading scores by about 5 points for both boys and girls, and the difference in treatment effects across gender is essentially zero and not statistically significant. Additionally, the difference in treatment effects across gender for math scores is negative but not statistically significant.

Next, we'll try by free lunch status, an indiactor of soci-economic status (SES).

```{r}
# Testing if the effect differs by Free Lunch status (SES) - reading
model_het <- lm_robust(read3 ~ D3 * lunch3, data = STAR_binary)
summary(model_het)

# Testing if the effect differs by Free Lunch status (SES) - math
model_het <- lm_robust(math3 ~ D3 * lunch3, data = STAR_binary)
summary(model_het)
```

Here, the interaction is nearly zero, and not statistically significant. This suggests that while free lunch status is a huge predictor of scores (the -19.48 drop), it doesn't actually change how effective the small classroom size is. Math scores for the third grade have a negative effect, again not statistically significant. 

Lastly we'll try by ethnicity using lm_lin.

```{r}
# Using lm_lin for the ethnicity heterogeneity check
model_lin_eth <- lm_lin(
  formula = read3 ~ D3, 
  covariates = ~ ethnicity, 
  data = STAR_binary
)

summary(model_lin_eth)
```

The coefficient in front of the interaction term for treatment and asian ethnicity is -28, a statistically significant and large negative effect. It suggests the treatment was much less effective on reading scores for the Asian students in this specific sample.

While there is evidence of heterogeneity—specifically a lower observed effect for Asian students (p=0.008)—small subgroup sizes for certain ethnicities (notably Hispanic students) lead to rank deficiency, suggesting that subgroup-specific results should be interpreted with caution.

{{< pagebreak >}}

# For troubleshooting: do not edit or remove

```{r}
#| echo: false
Sys.info()
Sys.time()
```

